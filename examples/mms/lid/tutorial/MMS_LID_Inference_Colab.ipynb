{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running MMS-LID inference in Colab"
      ],
      "metadata": {
        "id": "Rhm7khm6GskV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Clone fairseq-py and install latest version"
      ],
      "metadata": {
        "id": "2GfxksHDGyJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj2x80SegRzr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "\n",
        "# Change current working directory\n",
        "!pwd\n",
        "%cd \"/content/fairseq\"\n",
        "!pip install --editable ./\n",
        "!pip install tensorboardX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download MMS-LID model\n",
        "\n"
      ],
      "metadata": {
        "id": "cyk4JvZOHSw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = [\"l126\", \"l256\", \"l512\", \"l1024\", \"l2048\", \"l4017\"]\n",
        "\n",
        "# We will use L126 model which can recognize 126 languages\n",
        "model_name = available_models[0] # l126\n",
        "print(f\"Using model - {model_name}\")\n",
        "print(f\"Visit https://dl.fbaipublicfiles.com/mms/lid/mms1b_{model_name}_langs.html to check all the languages supported by this model.\")\n",
        "\n",
        "! mkdir -p /content/models_lid\n",
        "!wget -P /content/models_lid/{model_name} 'https://dl.fbaipublicfiles.com/mms/lid/mms1b_{model_name}.pt'\n",
        "!wget -P /content/models_lid/{model_name} 'https://dl.fbaipublicfiles.com/mms/lid/dict/l126/dict.lang.txt'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZ9WG85gZId",
        "outputId": "c93efb4f-6082-41da-ed49-62f52ecda0e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model - l126\n",
            "Visit https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126_langs.html to check all the languages supported by this model.\n",
            "--2024-12-25 02:01:32--  https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3856229421 (3.6G) [binary/octet-stream]\n",
            "Saving to: ‘/content/models_lid/l126/mms1b_l126.pt’\n",
            "\n",
            "mms1b_l126.pt       100%[===================>]   3.59G  21.2MB/s    in 2m 27s  \n",
            "\n",
            "2024-12-25 02:04:00 (25.0 MB/s) - ‘/content/models_lid/l126/mms1b_l126.pt’ saved [3856229421/3856229421]\n",
            "\n",
            "--2024-12-25 02:04:00--  https://dl.fbaipublicfiles.com/mms/lid/dict/l126/dict.lang.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.59, 13.227.219.70, 13.227.219.10, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 882 [text/plain]\n",
            "Saving to: ‘/content/models_lid/l126/dict.lang.txt’\n",
            "\n",
            "dict.lang.txt       100%[===================>]     882  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-25 02:04:01 (47.6 MB/s) - ‘/content/models_lid/l126/dict.lang.txt’ saved [882/882]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare manifest files\n",
        "Create a folder on path '/content/audio_samples/' and upload your .wav audio files that you need to recognize e.g. '/content/audio_samples/abc.wav' , '/content/audio_samples/def.wav' etc...\n",
        "\n",
        "Note: You need to make sure that the audio data you are using has a sample rate of 16kHz You can easily do this with FFMPEG like the example below that converts .mp3 file to .flac and fixing the audio sample rate\n",
        "\n",
        "Here, we use three examples - one audio file from English, Hindi, Chinese each."
      ],
      "metadata": {
        "id": "3p5-TQvKHXjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/audio_samples/\n",
        "for key in [\"en_us\", \"hi_in\", \"cmn_hans_cn\"]:\n",
        "  !wget -O /content/audio_samples/tmp.mp3 https://datasets-server.huggingface.co/assets/google/fleurs/--/{key}/train/0/audio/audio.mp3\n",
        "  !ffmpeg -hide_banner -loglevel error -y -i   /content/audio_samples/tmp.mp3 -ar 16000 /content/audio_samples/{key}.wav\n",
        "\n",
        "! mkdir -p /content/audio_samples/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnim4bokprbB",
        "outputId": "01ee8e08-1309-403e-f043-d6b4f365bcc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-25 02:04:50--  https://datasets-server.huggingface.co/assets/google/fleurs/--/en_us/train/0/audio/audio.mp3\n",
            "Resolving datasets-server.huggingface.co (datasets-server.huggingface.co)... 18.239.94.9, 18.239.94.106, 18.239.94.90, ...\n",
            "Connecting to datasets-server.huggingface.co (datasets-server.huggingface.co)|18.239.94.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-12-25 02:04:50 ERROR 403: Forbidden.\n",
            "\n",
            "\u001b[0;35m[mp3 @ 0x555d7e9b4040] \u001b[0m\u001b[1;31mFailed to read frame size: Could not seek to 1026.\n",
            "\u001b[0m\u001b[1;31m/content/audio_samples/tmp.mp3: Invalid argument\n",
            "\u001b[0m--2024-12-25 02:04:50--  https://datasets-server.huggingface.co/assets/google/fleurs/--/hi_in/train/0/audio/audio.mp3\n",
            "Resolving datasets-server.huggingface.co (datasets-server.huggingface.co)... 18.239.94.9, 18.239.94.106, 18.239.94.90, ...\n",
            "Connecting to datasets-server.huggingface.co (datasets-server.huggingface.co)|18.239.94.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-12-25 02:04:50 ERROR 403: Forbidden.\n",
            "\n",
            "\u001b[0;35m[mp3 @ 0x57e6bf65c040] \u001b[0m\u001b[1;31mFailed to read frame size: Could not seek to 1026.\n",
            "\u001b[0m\u001b[1;31m/content/audio_samples/tmp.mp3: Invalid argument\n",
            "\u001b[0m--2024-12-25 02:04:51--  https://datasets-server.huggingface.co/assets/google/fleurs/--/cmn_hans_cn/train/0/audio/audio.mp3\n",
            "Resolving datasets-server.huggingface.co (datasets-server.huggingface.co)... 18.239.94.9, 18.239.94.106, 18.239.94.90, ...\n",
            "Connecting to datasets-server.huggingface.co (datasets-server.huggingface.co)|18.239.94.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-12-25 02:04:51 ERROR 403: Forbidden.\n",
            "\n",
            "\u001b[0;35m[mp3 @ 0x55decd595040] \u001b[0m\u001b[1;31mFailed to read frame size: Could not seek to 1026.\n",
            "\u001b[0m\u001b[1;31m/content/audio_samples/tmp.mp3: Invalid argument\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/manifest/\n",
        "import os\n",
        "with open(\"/content/manifest/dev.tsv\", \"w\") as ftsv, open(\"/content/manifest/dev.lang\", \"w\") as flang:\n",
        "  ftsv.write(\"/\\n\")\n",
        "\n",
        "  for fl in os.listdir(\"/content/audio_samples/\"):\n",
        "    if not fl.endswith(\".wav\"):\n",
        "      continue\n",
        "    audio_path = f\"/content/audio_samples/{fl}\"\n",
        "    # duration should be number of samples in audio. For inference, using a random value should be fine.\n",
        "    duration = 1234\n",
        "    ftsv.write(f\"{audio_path}\\t{duration}\\n\")\n",
        "    flang.write(\"eng\\n\") # This is the \"true\" language for the audio. For inference, using a random value should be fine.\n"
      ],
      "metadata": {
        "id": "C2QcjRT-BArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Run Inference and transcribe your audio(s)\n"
      ],
      "metadata": {
        "id": "44UvHjmMI28Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/fairseq\"\n",
        "os.environ[\"PREFIX\"] = \"INFER\"\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "os.environ[\"USER\"] = \"mms_lid_user\"\n",
        "\n",
        "!python3 examples/mms/lid/infer.py /content/models_lid/{model_name} --path /content/models_lid/{model_name}/mms1b_l126.pt \\\n",
        "  --task audio_classification  --infer-manifest /content/manifest/dev.tsv --output-path /content/manifest/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8N1RKtBiw5V",
        "outputId": "09d3fe43-26a4-4f9b-c56d-d38b6d45cdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-25 18:19:19.545731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-25 18:19:21.567795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "| loading model from /content/models_lid/l126/mms1b_l126.pt\n",
            "2023-05-25 18:19:29 | INFO | fairseq.tasks.audio_classification | Using dict_path : /content/models_lid/l126/dict.lang.txt\n",
            "2023-05-25 18:19:29 | INFO | root | === Number of labels = 126\n",
            "2023-05-25 18:20:01 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 3, skipped 0 samples\n",
            "2023-05-25 18:20:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-25 18:20:01 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-25 18:20:01 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
            "2023-05-25 18:20:01 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
            "2023-05-25 18:20:01 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "3it [00:07,  2.61s/it]\n",
            "Outputs will be located at - /content/manifest//predictions.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- INPUT FILES -----\")\n",
        "! tail -n +2 /content/manifest/dev.tsv\n",
        "\n",
        "print(\"\\n----- TOP-K PREDICTONS WITH SCORE -----\")\n",
        "! cat /content/manifest//predictions.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f7FROqiC-2z",
        "outputId": "3a28ceee-dbb7-4810-f9ca-d11b14a8340b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- INPUT FILES -----\n",
            "/content/audio_samples/hi_in.wav\t1234\n",
            "/content/audio_samples/en_us.wav\t1234\n",
            "/content/audio_samples/cmn_hans_cn.wav\t1234\n",
            "\n",
            "----- TOP-K PREDICTONS WITH SCORE -----\n",
            "[[\"hin\", 0.9931250810623169], [\"urd\", 0.005808886140584946], [\"snd\", 0.0005312535213306546]]\n",
            "[[\"eng\", 0.9989539980888367], [\"fas\", 0.00036296260077506304], [\"haw\", 7.031611312413588e-05]]\n",
            "[[\"cmn\", 0.9996059536933899], [\"bod\", 0.0002111078501911834], [\"kor\", 9.211552242049947e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzHHmno5DZC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}